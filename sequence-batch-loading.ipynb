{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well done is better than well said',\n",
       " 'better slip with foot than tongue',\n",
       " 'there never was a good war or a bad peace']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1 = \"Well done is better than well said\".lower()\n",
    "q2 = \"Better slip with foot than tongue\".lower()\n",
    "q3 = \"There never was a good war or a bad peace\".lower()\n",
    "quotes = [q1, q2, q3]\n",
    "quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = 0\n",
    "s2 = 1\n",
    "s3 = 2\n",
    "sources = [s1, s2, s3]\n",
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23 unique characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<pad>',\n",
       " '<eos>',\n",
       " '<unk>',\n",
       " 'c',\n",
       " 'f',\n",
       " 'w',\n",
       " 'a',\n",
       " 'p',\n",
       " 'h',\n",
       " 'v',\n",
       " 'l',\n",
       " 'r',\n",
       " ' ',\n",
       " 'u',\n",
       " 'i',\n",
       " 'e',\n",
       " 'g',\n",
       " 'd',\n",
       " 'n',\n",
       " 's',\n",
       " 'o',\n",
       " 't',\n",
       " 'b']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = ['<pad>', '<eos>', '<unk>'] + list(set(' '.join(quotes)))\n",
    "nb_char = len(chars)\n",
    "\n",
    "print(f'There are {nb_char} unique characters')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<eos>': 1,\n",
       " '<unk>': 2,\n",
       " 'c': 3,\n",
       " 'f': 4,\n",
       " 'w': 5,\n",
       " 'a': 6,\n",
       " 'p': 7,\n",
       " 'h': 8,\n",
       " 'v': 9,\n",
       " 'l': 10,\n",
       " 'r': 11,\n",
       " ' ': 12,\n",
       " 'u': 13,\n",
       " 'i': 14,\n",
       " 'e': 15,\n",
       " 'g': 16,\n",
       " 'd': 17,\n",
       " 'n': 18,\n",
       " 's': 19,\n",
       " 'o': 20,\n",
       " 't': 21,\n",
       " 'b': 22}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "char_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([34]), torch.Size([33]), torch.Size([41])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_seq = [torch.tensor(list(map(lambda char: char_to_ix[char], i))) for i in quotes]\n",
    "[x.shape for x in x_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([41]), torch.Size([41]), torch.Size([41])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_padded = pad_sequence(x_seq, batch_first=True, padding_value=char_to_ix['<pad>'])\n",
    "[x.shape for x in x_padded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 5, 15, 10, 10, 12, 17, 20, 18, 15, 12, 14, 19, 12, 22, 15, 21, 21, 15,\n",
       "         11, 12, 21,  8,  6, 18, 12,  5, 15, 10, 10, 12, 19,  6, 14, 17]),\n",
       " tensor([0]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class QuoteDataset(Dataset):\n",
    "    \"\"\"Dataset for Summarizing Benjamin Franklin Quotes\"\"\"\n",
    "    \n",
    "    def __init__(self, quotes, sources):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            quotes (list(string)): A list of quotes\n",
    "            sources (list(string)): A list of source ids\n",
    "        \"\"\"\n",
    "        self.quotes = quotes\n",
    "        self.sources = sources\n",
    "        \n",
    "        assert len(quotes) == len(sources), \"The number of quotes must match the number of sources!\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(quotes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        x_seq = torch.tensor(list(map(lambda char: char_to_ix[char], self.quotes[idx])))\n",
    "        y = torch.tensor([self.sources[idx]])\n",
    "            \n",
    "        return (x_seq, y)\n",
    "    \n",
    "dset = QuoteDataset(quotes, sources)\n",
    "dset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "    yy = torch.tensor(yy)\n",
    "    \n",
    "    return xx_pad, yy, x_lens\n",
    "\n",
    "dataset = QuoteDataset(quotes, sources)\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=2, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 34, 100])\n",
      "tensor([0, 1])\n",
      "torch.Size([1, 41, 100])\n",
      "tensor([2])\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(nb_char, 100)\n",
    "for i,(x_padded, y, x_lens) in enumerate(data_loader):\n",
    "    x_embed = embedding(x_padded)\n",
    "    print(x_embed.shape)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        embedding_dim = 100\n",
    "        hidden_size = 100\n",
    "        \n",
    "        self.embedding = nn.Embedding(nb_char, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, len(sources))\n",
    "        \n",
    "    def forward(self, x, x_lens):\n",
    "        x_embed = self.embedding(x)\n",
    "        x_packed = pack_padded_sequence(x_embed, x_lens, batch_first=True, enforce_sorted=False)\n",
    "        output_packed, hidden = self.gru(x_packed)\n",
    "        output_padded, output_lengths = pad_packed_sequence(output_packed, batch_first=True)\n",
    "        output = self.fc_out(output_padded[:, -1, :])\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Cross Entropy Loss: 0.973617434501648\n",
      "Batch Cross Entropy Loss: 1.0577738285064697\n",
      "Batch Cross Entropy Loss: 0.8178512454032898\n",
      "Batch Cross Entropy Loss: 0.7443614602088928\n",
      "Batch Cross Entropy Loss: 0.7431101202964783\n",
      "Batch Cross Entropy Loss: 0.4948538839817047\n",
      "Batch Cross Entropy Loss: 0.7500364780426025\n",
      "Batch Cross Entropy Loss: 1.5231322050094604\n",
      "Batch Cross Entropy Loss: 0.6639506220817566\n",
      "Batch Cross Entropy Loss: 0.240587517619133\n",
      "Batch Cross Entropy Loss: 0.6594473719596863\n",
      "Batch Cross Entropy Loss: 1.3392215967178345\n",
      "Batch Cross Entropy Loss: 0.6280755996704102\n",
      "Batch Cross Entropy Loss: 0.1610555350780487\n",
      "Batch Cross Entropy Loss: 0.6162017583847046\n",
      "Batch Cross Entropy Loss: 0.1374739110469818\n",
      "Batch Cross Entropy Loss: 0.6053919792175293\n",
      "Batch Cross Entropy Loss: 0.11427903920412064\n",
      "Batch Cross Entropy Loss: 0.5964523553848267\n",
      "Batch Cross Entropy Loss: 0.09237660467624664\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.001)\n",
    "for epoch in range(10):\n",
    "    for i,(x_padded, y, x_lens) in enumerate(data_loader):\n",
    "        # This whole section below probably belongs in its own model\n",
    "        optimizer.zero_grad()\n",
    "        output = rnn(x_padded, x_lens)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Batch Cross Entropy Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
