{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Sequence Batch Loading\n",
    "### By Noah Ziems\n",
    "\n",
    "As it turns out, RNNs tend to be really slow to train. This is mainly because they are tough to train in batches. You can only run one input at a time through the network. Its worth noting this is one of the big reasons Transformer-based architectures have performed so well compared to traditional sequence-based models. Thankfully, PyTorch has a bunch of built-in methods to help speed up training RNNs. In this notebook, we'll explore a few of those.\n",
    "\n",
    "First let's start off importing everything we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quotes\n",
    "\n",
    "Lets keep things really really simple for now. We're going to make a datset with only 3 data points! Each quote(split by chars) will be the input and the goal of the RNN will simply be to recognize the quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well done is better than well said',\n",
       " 'better slip with foot than tongue',\n",
       " 'there never was a good war or a bad peace']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1 = \"Well done is better than well said\".lower()\n",
    "q2 = \"Better slip with foot than tongue\".lower()\n",
    "q3 = \"There never was a good war or a bad peace\".lower()\n",
    "quotes = [q1, q2, q3]\n",
    "quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "Here are the sources of our quotes. Nothing much to say here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = 0\n",
    "s2 = 1\n",
    "s3 = 2\n",
    "sources = [s1, s2, s3]\n",
    "sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Alright heres where things get interesting. We're breaking our dataset into tokens that can be fed into our RNN.\n",
    "\n",
    "First we reserve some tokens like `<pad>`(padding), `<eos>`(end of sentence), `<unk>`(unknown). These are used to indicate to the RNN some things that may not be properly extracted from the regular tokens.\n",
    "\n",
    "Next, we join all the quotes together and create a set of chars out of them. A set simply means a collection with no repeating elements(we dont want the same character to have two different tokens).\n",
    "\n",
    "Lastly, we print them out as a sanity check to make sure everything looks right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23 unique characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<pad>',\n",
       " '<eos>',\n",
       " '<unk>',\n",
       " 'l',\n",
       " 'i',\n",
       " 's',\n",
       " 'd',\n",
       " 'n',\n",
       " 'g',\n",
       " 't',\n",
       " 'v',\n",
       " 'f',\n",
       " 'h',\n",
       " 'e',\n",
       " 'u',\n",
       " 'c',\n",
       " 'p',\n",
       " 'o',\n",
       " 'w',\n",
       " 'r',\n",
       " ' ',\n",
       " 'a',\n",
       " 'b']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = ['<pad>', '<eos>', '<unk>'] + list(set(' '.join(quotes)))\n",
    "nb_char = len(chars)\n",
    "\n",
    "print(f'There are {nb_char} unique characters')\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion Dictionaries\n",
    "To make sure we can easily convert `chars` -> `ids` and `ids` to `chars`, we create two different dictionaries. These are really useful for encoding any given sentence before its given to the RNN and decoding any sentence. In this notebook we don't decode any sentences, but its still a good habit to put this in here.\n",
    "\n",
    "Once again, lets print some things out so we can make sure everything looks right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<eos>': 1,\n",
       " '<unk>': 2,\n",
       " 'l': 3,\n",
       " 'i': 4,\n",
       " 's': 5,\n",
       " 'd': 6,\n",
       " 'n': 7,\n",
       " 'g': 8,\n",
       " 't': 9,\n",
       " 'v': 10,\n",
       " 'f': 11,\n",
       " 'h': 12,\n",
       " 'e': 13,\n",
       " 'u': 14,\n",
       " 'c': 15,\n",
       " 'p': 16,\n",
       " 'o': 17,\n",
       " 'w': 18,\n",
       " 'r': 19,\n",
       " ' ': 20,\n",
       " 'a': 21,\n",
       " 'b': 22}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "char_to_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tensor Sequences\n",
    "Let's tokenize each of our quotes by converting each character into its corresponding id then putting all those ids into a pytorch vector. Lastly, we put each of those vectors in a list.\n",
    " Notice how the size of each tensor is the exact same as the length of each quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([34]), torch.Size([33]), torch.Size([41])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_seq = [torch.tensor(list(map(lambda char: char_to_ix[char], i))) for i in quotes]\n",
    "[x.shape for x in x_seq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Our Sequences\n",
    "When we send our batches to the GPU, they have to be sent as a single tensor. This means each quote has to be the same length when they get sent to the GPU. Unfortunately, not all our quotes are the same length. One is 34 chars, one is 33, and the last is 41. Therefore we have to pad each quote so they are all the same length. Thankfully, PyTorch has a function for this called `pad_sequence`.\n",
    "\n",
    "To use `pad_sequence` we simply give it our list of tokenized sequences from the last cell, tell it to pad after each sentence(instead of before), and tell it which value to pad with(in this case, our `<pad>` token.\n",
    "\n",
    "Now you can see that all of our padded sequences are the same length(the length of the longest quote). Time to move on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([41]), torch.Size([41]), torch.Size([41])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_padded = pad_sequence(x_seq, batch_first=True, padding_value=char_to_ix['<pad>'])\n",
    "[x.shape for x in x_padded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dataset\n",
    "\n",
    "Let's create a quick dataset so we can use the PyTorch Dataloader. I won't go over this in too much detail. Notice we aren't actually doing the padding in the dataset because `__getitem__` does not know about the other items that are being fetched, and therefore does not know what the longest sequence in the batch is. That is all done by the dataloader with the `pad_collate` function you see in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([18, 13,  3,  3, 20,  6, 17,  7, 13, 20,  4,  5, 20, 22, 13,  9,  9, 13,\n",
       "         19, 20,  9, 12, 21,  7, 20, 18, 13,  3,  3, 20,  5, 21,  4,  6]),\n",
       " tensor([0]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class QuoteDataset(Dataset):\n",
    "    \"\"\"Dataset for Summarizing Benjamin Franklin Quotes\"\"\"\n",
    "    \n",
    "    def __init__(self, quotes, sources):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            quotes (list(string)): A list of quotes\n",
    "            sources (list(string)): A list of source ids\n",
    "        \"\"\"\n",
    "        self.quotes = quotes\n",
    "        self.sources = sources\n",
    "        \n",
    "        assert len(quotes) == len(sources), \"The number of quotes must match the number of sources!\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(quotes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        x_seq = torch.tensor(list(map(lambda char: char_to_ix[char], self.quotes[idx])))\n",
    "        y = torch.tensor([self.sources[idx]])\n",
    "            \n",
    "        return (x_seq, y)\n",
    "    \n",
    "dset = QuoteDataset(quotes, sources)\n",
    "dset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Each Batch with the DataLoader\n",
    "To pad each batch the way we want, we have to write a custom `collate_fn` which is used to combine each item in the batch. It's worth noting that we have to return the length of each input sequence(`x_lens`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=char_to_ix['<pad>'])\n",
    "    yy = torch.tensor(yy)\n",
    "    \n",
    "    return xx_pad, yy, x_lens\n",
    "\n",
    "dataset = QuoteDataset(quotes, sources)\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=2, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding and DataLoader Testing\n",
    "A PyTorch embedding is used to convert token ids into a representation vector. These representation vectors are not really human interpretable, but they have some really fascinating qualities that I will not go over here.\n",
    "\n",
    "Let's make an embedding that represents each word as a 100-dim vector.\n",
    "\n",
    "Then, lets enumerate through the dataset as we would during training to make sure the shapes all look right. We know the batch size of our dataloader is 2, meaning it fetches two sequences at a time. We also know that our entire dataset only has three sequences. This means our first batch should have two sequences and our second batch should have only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 41, 100])\n",
      "tensor([2, 1])\n",
      "torch.Size([1, 34, 100])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(nb_char, 100)\n",
    "for i,(x_padded, y, x_lens) in enumerate(data_loader):\n",
    "    x_embed = embedding(x_padded)\n",
    "    print(x_embed.shape)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks good! As you can see, the first batch returned two sequences(the first number in the shape) each with 41 tokens with each token being represented as a 100-dimension vector.\n",
    "\n",
    "The second batch has only one sequence with a length of 34.\n",
    "Now let's see how this would be used in practice.\n",
    "\n",
    "### Creating an RNN\n",
    "Let's create an RNN that can handle GPU optimized batching that we showed above. The most important things here to note are the `pack_padded_sequence` here before it is fed into the GRU and `pad_packed_sequence` after it is returned from the GRU. This simply opimizes the sequence representation before it is sent to the GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        embedding_dim = 100\n",
    "        hidden_size = 100\n",
    "        \n",
    "        self.embedding = nn.Embedding(nb_char, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_size, len(sources))\n",
    "        \n",
    "    def forward(self, x, x_lens):\n",
    "        x_embed = self.embedding(x)\n",
    "        x_packed = pack_padded_sequence(x_embed, x_lens, batch_first=True, enforce_sorted=False)\n",
    "        output_packed, hidden = self.gru(x_packed)\n",
    "        output_padded, output_lengths = pad_packed_sequence(output_packed, batch_first=True)\n",
    "        output = self.fc_out(output_padded[:, -1, :])\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for a Few Batches\n",
    "Now that we have everything setup, lets train the network for a few batches to show that its working and how easy it is to use the RNN now. This is almost identical to the simplicity of training a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Cross Entropy Loss: 1.1170361042022705\n",
      "Batch Cross Entropy Loss: 1.2228976488113403\n",
      "Batch Cross Entropy Loss: 1.0215411186218262\n",
      "Batch Cross Entropy Loss: 1.3039233684539795\n",
      "Batch Cross Entropy Loss: 0.8585443496704102\n",
      "Batch Cross Entropy Loss: 1.1366260051727295\n",
      "Batch Cross Entropy Loss: 0.775987982749939\n",
      "Batch Cross Entropy Loss: 0.9278870820999146\n",
      "Batch Cross Entropy Loss: 0.8335872888565063\n",
      "Batch Cross Entropy Loss: 0.39104533195495605\n",
      "Batch Cross Entropy Loss: 0.7986896634101868\n",
      "Batch Cross Entropy Loss: 0.32837775349617004\n",
      "Batch Cross Entropy Loss: 0.6772658824920654\n",
      "Batch Cross Entropy Loss: 0.4134480059146881\n",
      "Batch Cross Entropy Loss: 0.7113977670669556\n",
      "Batch Cross Entropy Loss: 0.20679523050785065\n",
      "Batch Cross Entropy Loss: 0.6638545989990234\n",
      "Batch Cross Entropy Loss: 0.1615143120288849\n",
      "Batch Cross Entropy Loss: 0.6007986664772034\n",
      "Batch Cross Entropy Loss: 0.1702224612236023\n",
      "Batch Cross Entropy Loss: 0.5839599370956421\n",
      "Batch Cross Entropy Loss: 0.12439825385808945\n",
      "Batch Cross Entropy Loss: 0.5824906229972839\n",
      "Batch Cross Entropy Loss: 0.07569334656000137\n",
      "Batch Cross Entropy Loss: 0.5633173584938049\n",
      "Batch Cross Entropy Loss: 0.0665641576051712\n",
      "Batch Cross Entropy Loss: 0.5579869747161865\n",
      "Batch Cross Entropy Loss: 0.04968152195215225\n",
      "Batch Cross Entropy Loss: 0.5513321161270142\n",
      "Batch Cross Entropy Loss: 0.03928016498684883\n",
      "Batch Cross Entropy Loss: 0.5458254814147949\n",
      "Batch Cross Entropy Loss: 0.4440343677997589\n",
      "Batch Cross Entropy Loss: 0.5423186421394348\n",
      "Batch Cross Entropy Loss: 0.031414516270160675\n",
      "Batch Cross Entropy Loss: 0.5420676469802856\n",
      "Batch Cross Entropy Loss: 0.3117572069168091\n",
      "Batch Cross Entropy Loss: 0.5376949310302734\n",
      "Batch Cross Entropy Loss: 0.026515353471040726\n",
      "Batch Cross Entropy Loss: 0.5396315455436707\n",
      "Batch Cross Entropy Loss: 0.017436427995562553\n",
      "Batch Cross Entropy Loss: 0.539127767086029\n",
      "Batch Cross Entropy Loss: 0.14553320407867432\n",
      "Batch Cross Entropy Loss: 0.5335733890533447\n",
      "Batch Cross Entropy Loss: 0.021645497530698776\n",
      "Batch Cross Entropy Loss: 0.5361558794975281\n",
      "Batch Cross Entropy Loss: 0.013257095590233803\n",
      "Batch Cross Entropy Loss: 0.5316089987754822\n",
      "Batch Cross Entropy Loss: 0.01911376230418682\n",
      "Batch Cross Entropy Loss: 0.5306808352470398\n",
      "Batch Cross Entropy Loss: 0.01796170324087143\n",
      "Batch Cross Entropy Loss: 0.5363677740097046\n",
      "Batch Cross Entropy Loss: 0.05686682462692261\n",
      "Batch Cross Entropy Loss: 0.5320026278495789\n",
      "Batch Cross Entropy Loss: 0.01030717696994543\n",
      "Batch Cross Entropy Loss: 0.5354399681091309\n",
      "Batch Cross Entropy Loss: 0.04405243694782257\n",
      "Batch Cross Entropy Loss: 0.5279814004898071\n",
      "Batch Cross Entropy Loss: 0.014061777852475643\n",
      "Batch Cross Entropy Loss: 0.5274264812469482\n",
      "Batch Cross Entropy Loss: 0.013301562517881393\n",
      "Batch Cross Entropy Loss: 0.52886962890625\n",
      "Batch Cross Entropy Loss: 0.008691576309502125\n",
      "Batch Cross Entropy Loss: 0.5340831279754639\n",
      "Batch Cross Entropy Loss: 0.029825976118445396\n",
      "Batch Cross Entropy Loss: 0.5274255871772766\n",
      "Batch Cross Entropy Loss: 0.00810985453426838\n",
      "Batch Cross Entropy Loss: 0.5267623662948608\n",
      "Batch Cross Entropy Loss: 0.007840808480978012\n",
      "Batch Cross Entropy Loss: 0.5333877801895142\n",
      "Batch Cross Entropy Loss: 0.024205399677157402\n",
      "Batch Cross Entropy Loss: 0.5255690217018127\n",
      "Batch Cross Entropy Loss: 0.007341549266129732\n",
      "Batch Cross Entropy Loss: 0.5250000953674316\n",
      "Batch Cross Entropy Loss: 0.007110173348337412\n",
      "Batch Cross Entropy Loss: 0.5328584909439087\n",
      "Batch Cross Entropy Loss: 0.020404083654284477\n",
      "Batch Cross Entropy Loss: 0.5228508710861206\n",
      "Batch Cross Entropy Loss: 0.008826171979308128\n",
      "Batch Cross Entropy Loss: 0.5223920941352844\n",
      "Batch Cross Entropy Loss: 0.008512402884662151\n",
      "Batch Cross Entropy Loss: 0.5218753218650818\n",
      "Batch Cross Entropy Loss: 0.008195225149393082\n",
      "Batch Cross Entropy Loss: 0.5213109850883484\n",
      "Batch Cross Entropy Loss: 0.0078801941126585\n",
      "Batch Cross Entropy Loss: 0.5215262174606323\n",
      "Batch Cross Entropy Loss: 0.0060154106467962265\n",
      "Batch Cross Entropy Loss: 0.5325944423675537\n",
      "Batch Cross Entropy Loss: 0.01613616943359375\n",
      "Batch Cross Entropy Loss: 0.5196263194084167\n",
      "Batch Cross Entropy Loss: 0.007026130799204111\n",
      "Batch Cross Entropy Loss: 0.519113302230835\n",
      "Batch Cross Entropy Loss: 0.00678153894841671\n",
      "Batch Cross Entropy Loss: 0.5325548052787781\n",
      "Batch Cross Entropy Loss: 0.014687508344650269\n",
      "Batch Cross Entropy Loss: 0.5324118137359619\n",
      "Batch Cross Entropy Loss: 0.014161333441734314\n",
      "Batch Cross Entropy Loss: 0.5183655619621277\n",
      "Batch Cross Entropy Loss: 0.005305140744894743\n",
      "Batch Cross Entropy Loss: 0.5319517850875854\n",
      "Batch Cross Entropy Loss: 0.013045676052570343\n",
      "Batch Cross Entropy Loss: 0.5173957943916321\n",
      "Batch Cross Entropy Loss: 0.005740347784012556\n",
      "Batch Cross Entropy Loss: 0.5173749327659607\n",
      "Batch Cross Entropy Loss: 0.005017783492803574\n",
      "Batch Cross Entropy Loss: 0.5169333815574646\n",
      "Batch Cross Entropy Loss: 0.00491933012381196\n",
      "Batch Cross Entropy Loss: 0.5313957929611206\n",
      "Batch Cross Entropy Loss: 0.011320053599774837\n",
      "Batch Cross Entropy Loss: 0.5158688426017761\n",
      "Batch Cross Entropy Loss: 0.005147062707692385\n",
      "Batch Cross Entropy Loss: 0.5311658978462219\n",
      "Batch Cross Entropy Loss: 0.010610122233629227\n",
      "Batch Cross Entropy Loss: 0.5309325456619263\n",
      "Batch Cross Entropy Loss: 0.010230954736471176\n",
      "Batch Cross Entropy Loss: 0.5305671095848083\n",
      "Batch Cross Entropy Loss: 0.009813870303332806\n",
      "Batch Cross Entropy Loss: 0.5151803493499756\n",
      "Batch Cross Entropy Loss: 0.004409827757626772\n",
      "Batch Cross Entropy Loss: 0.514984667301178\n",
      "Batch Cross Entropy Loss: 0.0043356469832360744\n",
      "Batch Cross Entropy Loss: 0.5146775841712952\n",
      "Batch Cross Entropy Loss: 0.0042522018775343895\n",
      "Batch Cross Entropy Loss: 0.5142797827720642\n",
      "Batch Cross Entropy Loss: 0.004162220750004053\n",
      "Batch Cross Entropy Loss: 0.5295858383178711\n",
      "Batch Cross Entropy Loss: 0.008286735974252224\n",
      "Batch Cross Entropy Loss: 0.5135028958320618\n",
      "Batch Cross Entropy Loss: 0.003985083196312189\n",
      "Batch Cross Entropy Loss: 0.5293830037117004\n",
      "Batch Cross Entropy Loss: 0.007840453647077084\n",
      "Batch Cross Entropy Loss: 0.5128558874130249\n",
      "Batch Cross Entropy Loss: 0.00382382795214653\n",
      "Batch Cross Entropy Loss: 0.5125032663345337\n",
      "Batch Cross Entropy Loss: 0.003744971938431263\n",
      "Batch Cross Entropy Loss: 0.5290718674659729\n",
      "Batch Cross Entropy Loss: 0.00723764393478632\n",
      "Batch Cross Entropy Loss: 0.5288949012756348\n",
      "Batch Cross Entropy Loss: 0.007038559764623642\n",
      "Batch Cross Entropy Loss: 0.5116449594497681\n",
      "Batch Cross Entropy Loss: 0.0035286799538880587\n",
      "Batch Cross Entropy Loss: 0.5112387537956238\n",
      "Batch Cross Entropy Loss: 0.003735352074727416\n",
      "Batch Cross Entropy Loss: 0.528330385684967\n",
      "Batch Cross Entropy Loss: 0.006473997142165899\n",
      "Batch Cross Entropy Loss: 0.5280969142913818\n",
      "Batch Cross Entropy Loss: 0.006294900085777044\n",
      "Batch Cross Entropy Loss: 0.5105414390563965\n",
      "Batch Cross Entropy Loss: 0.0035437659826129675\n",
      "Batch Cross Entropy Loss: 0.527518093585968\n",
      "Batch Cross Entropy Loss: 0.005934357643127441\n",
      "Batch Cross Entropy Loss: 0.5102002024650574\n",
      "Batch Cross Entropy Loss: 0.003415823681280017\n",
      "Batch Cross Entropy Loss: 0.5100621581077576\n",
      "Batch Cross Entropy Loss: 0.0031607216224074364\n",
      "Batch Cross Entropy Loss: 0.5268716216087341\n",
      "Batch Cross Entropy Loss: 0.0054825181141495705\n",
      "Batch Cross Entropy Loss: 0.5266240835189819\n",
      "Batch Cross Entropy Loss: 0.005341069307178259\n",
      "Batch Cross Entropy Loss: 0.5262500643730164\n",
      "Batch Cross Entropy Loss: 0.0051871477626264095\n",
      "Batch Cross Entropy Loss: 0.5093458890914917\n",
      "Batch Cross Entropy Loss: 0.0031226943247020245\n",
      "Batch Cross Entropy Loss: 0.5092036724090576\n",
      "Batch Cross Entropy Loss: 0.0030696913599967957\n",
      "Batch Cross Entropy Loss: 0.5089861750602722\n",
      "Batch Cross Entropy Loss: 0.0029270683880895376\n",
      "Batch Cross Entropy Loss: 0.5252416729927063\n",
      "Batch Cross Entropy Loss: 0.004683596082031727\n",
      "Batch Cross Entropy Loss: 0.5083515644073486\n",
      "Batch Cross Entropy Loss: 0.0029105464927852154\n",
      "Batch Cross Entropy Loss: 0.5080215334892273\n",
      "Batch Cross Entropy Loss: 0.0028609796427190304\n",
      "Batch Cross Entropy Loss: 0.5249220728874207\n",
      "Batch Cross Entropy Loss: 0.004405436106026173\n",
      "Batch Cross Entropy Loss: 0.5247457027435303\n",
      "Batch Cross Entropy Loss: 0.004313569515943527\n",
      "Batch Cross Entropy Loss: 0.5072206854820251\n",
      "Batch Cross Entropy Loss: 0.0027183268684893847\n",
      "Batch Cross Entropy Loss: 0.5242560505867004\n",
      "Batch Cross Entropy Loss: 0.004123162943869829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Cross Entropy Loss: 0.5068260431289673\n",
      "Batch Cross Entropy Loss: 0.0026532942429184914\n",
      "Batch Cross Entropy Loss: 0.523768961429596\n",
      "Batch Cross Entropy Loss: 0.003945306409150362\n",
      "Batch Cross Entropy Loss: 0.5234556794166565\n",
      "Batch Cross Entropy Loss: 0.003855534829199314\n",
      "Batch Cross Entropy Loss: 0.5230298638343811\n",
      "Batch Cross Entropy Loss: 0.0037595797330141068\n",
      "Batch Cross Entropy Loss: 0.5225135684013367\n",
      "Batch Cross Entropy Loss: 0.0036596960853785276\n",
      "Batch Cross Entropy Loss: 0.506630003452301\n",
      "Batch Cross Entropy Loss: 0.002454365836456418\n",
      "Batch Cross Entropy Loss: 0.5065847635269165\n",
      "Batch Cross Entropy Loss: 0.002421425189822912\n",
      "Batch Cross Entropy Loss: 0.521290123462677\n",
      "Batch Cross Entropy Loss: 0.003401329508051276\n",
      "Batch Cross Entropy Loss: 0.5062876343727112\n",
      "Batch Cross Entropy Loss: 0.0024307011626660824\n",
      "Batch Cross Entropy Loss: 0.5061134099960327\n",
      "Batch Cross Entropy Loss: 0.002318434417247772\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.001)\n",
    "for epoch in range(100):\n",
    "    for i,(x_padded, y, x_lens) in enumerate(data_loader):\n",
    "        # This whole section below probably belongs in its own model\n",
    "        optimizer.zero_grad()\n",
    "        output = rnn(x_padded, x_lens)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Batch Cross Entropy Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "It all looks good! Thanks for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
